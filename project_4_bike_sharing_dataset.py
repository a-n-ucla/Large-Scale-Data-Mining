# -*- coding: utf-8 -*-
"""Project 4: Bike Sharing Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZCEr5h9EoDB59uZT80ucyj7J_sju3Q21
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import metrics
from sklearn import preprocessing
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

# Configuring plotting visual and sizes
sns.set_style('whitegrid')
sns.set_context('talk')
params = {'legend.fontsize': 'small',
          'figure.figsize' : (20, 10),
          'axes.labelsize' : 'small',
          'axes.titlesize' : 'small',
          'xtick.labelsize': 'small',
          'ytick.labelsize': 'small'}

plt.rcParams.update(params)

import io
day_df = pd.read_csv('day.csv')

day_df.info()

drop_features     = ['instant', 'dteday'    ]
category_features_day = ['season' , 'yr'        , 'mnth', 'holiday'  , 'weekday', 'workingday', 'weathersit']
number_features_day   = ['temp'   , 'atemp'     , 'hum' , 'windspeed']
target_features_day   = ['casual' , 'registered', 'cnt' ]

features          = category_features_day + number_features_day

print(day_df[category_features_day].astype('category').describe())

print(day_df[number_features_day].describe())

# Checking for missing entries
print(day_df.isnull().any())

day_df_drop = day_df.drop(drop_features, axis=1)

day_df_drop.info()
corrMatt_full = day_df_drop.corr()

corrMatt = corrMatt_full.iloc[::-1, -1:-4:-1]
mask = np.array(corrMatt)

mask[np.tril_indices_from(mask)] = False
fig,ax = plt.subplots()
heatmap = sns.heatmap(corrMatt,
            mask=mask,
            vmax=0.8,
            vmin=-0.8,
            cmap='coolwarm',
#            square=True,
            annot = True,
            ax=ax)

"""# **Question 1**

*   Temperature ("temp" and "atemp") has the highest absolute correlation with count ("cnt"). This makes sense considering people don't prefer staying outdoors for longer periods of time when it is hot, and thus would want to move themselves quicker, prefering bikes over having to walk for short distances.

*   Year ("yr") has the highest absolute correlation with the number of registered users ("registered"). This makes some sense when you consider that most of the registered users might have been casual users in the past, i.e. they must have used the service before choosing to register.

*   Working-Day ("workingday") has high negative correlation with casual ("registered"), implying that the count of casual users is relatively much lower on working-days, maybe most of the casual users don't use this service to commute to work.
"""

fig, axes = plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(15, 15)
axes[0][0].hist(day_df[number_features_day[0]], bins=10, range=(0.0,1.0))
axes[0][1].hist(day_df[number_features_day[1]], bins=10, range=(0.0,1.0))
axes[1][0].hist(day_df[number_features_day[2]], bins=10, range=(0.0,1.0))
axes[1][1].hist(day_df[number_features_day[3]], bins=10, range=(0.0,1.0))

txt = 'Histogram of '

axes[0][0].set(xlabel=number_features_day[0],title= txt + number_features_day[0])
axes[0][1].set(xlabel=number_features_day[1],title= txt + number_features_day[1])
axes[1][0].set(xlabel=number_features_day[2],title= txt + number_features_day[2])
axes[1][1].set(xlabel=number_features_day[3],title= txt + number_features_day[3])

"""# **Question 2**

Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. For preprocessing skewed data, there are many data transformation methods:
* Logarithm transformation:
The logarithm, x to log base 10 of x, or x to log base e of x (ln x), or x to log base 2 of x, is a strong transformation and can be used to reduce right skewness. Log transformation approximates the data to a normal distribution.
* Cube root transformation:
The cube root transformation involves converting x to x^(1/3). This is a fairly strong transformation with a substantial effect on distribution shape: but is weaker than the logarithm. 
* Z score normalization: this method subtracts the mean ($\mu$) and scales by the standard deviation ($\sigma$).
$z = (x - \mu)/\sigma$
"""

tgt = 0

fig, axes = plt.subplots(nrows=2,ncols=3)
fig.set_size_inches(25, 15)
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[0],orient="v",ax=axes[0][0])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[1],orient="v",ax=axes[0][1])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[2],orient="v",ax=axes[0][2])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[4],orient="v",ax=axes[1][0])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[5],orient="v",ax=axes[1][1])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[6],orient="v",ax=axes[1][2])

txt = 'Box-plot of '

axes[0][0].set(xlabel=category_features_day[0], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[0])
axes[0][1].set(xlabel=category_features_day[1], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[1])
axes[0][2].set(xlabel=category_features_day[2], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[2])
axes[1][0].set(xlabel=category_features_day[4], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[4])
axes[1][1].set(xlabel=category_features_day[5], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[5])
axes[1][2].set(xlabel=category_features_day[6], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[6])

"""# **Question 3**

Features vs. casual users
* 'season': average number of casual bikers is highest in summer, followed by spring, fall and winter. This makes sense because it will not be comfortable to ride in the cold.
* 'year': casual riders have increased in the 2012, as more people become aware of bike sharing. 
* 'month': bike riders are high in Mar-Oct, but much less in Nov-Feb because of the winter season.
* 'weekday': casual riders are high during the weekend, but low during  the weekdays. This means the casual riders are not using the bike sharing service to commute to work, only for leisure.
* 'working day': As expected from the 'weekday' plot, the casual riders are much less during the working day because the casual riders don't use the service to commute to work.
* 'weather situation': The weather situation affects the number of riders because it is not comfortable to ride on rainy, snowy conditions.

Features vs. registered
* 'season': average number of registered bikers is highest in summer, followed by spring, fall and winter - similar to casual users. This makes sense because it will not be comfortable to ride in the cold.
* 'year': registered riders have increased in the 2012, as more people become aware of bike sharing - again similar to casual users.
* 'month': registered bike riders are high in Mar-Oct, when holidays are less, but much less in Nov-Feb because of the holiday season (Christmas, New Year).
* 'weekday': registered riders are low during the weekend, but high during  the weekdays. This means these riders have registered for the bike sharing service to commute to work.
* 'working day': As expected from the 'weekday' plot, the registered riders are much less during the weekends because the registered riders use the service to commute to work.
* 'weather situation': The weather situation affects the number of riders because it is not comfortable to ride on rainy, snowy conditions.

Features vs. count
* 'season': average number of total bikers is highest in summer, followed by spring, fall and winter - similar to casual, registered users. This makes sense because it will not be comfortable to ride in the cold irrespective of whether the rider has registered or not.
* 'year': total number of riders have increased in the 2012, as more people become aware of bike sharing.
* 'month': registered bike riders are high in Mar-Oct, when holidays are less, but much less in Nov-Feb because of the holiday season (Christmas, New Year).
* 'weekday': registered riders are low during the weekend, but high during  the weekdays. This means these riders have registered for the bike sharing service to commute to work.
* 'working day': As expected from the 'weekday' plot, the registered riders are much less during the weekends because the registered riders use the service to commute to work.
* 'weather situation': The weather situation affects the number of riders because it is not comfortable to ride on rainy, snowy conditions.
"""

tgt = 1

fig, axes = plt.subplots(nrows=2,ncols=3)
fig.set_size_inches(25, 15)
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[0],orient="v",ax=axes[0][0])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[1],orient="v",ax=axes[0][1])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[2],orient="v",ax=axes[0][2])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[4],orient="v",ax=axes[1][0])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[5],orient="v",ax=axes[1][1])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[6],orient="v",ax=axes[1][2])

txt = 'Box Plot on '

axes[0][0].set(xlabel=category_features_day[0], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[0])
axes[0][1].set(xlabel=category_features_day[1], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[1])
axes[0][2].set(xlabel=category_features_day[2], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[2])
axes[1][0].set(xlabel=category_features_day[4], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[4])
axes[1][1].set(xlabel=category_features_day[5], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[5])
axes[1][2].set(xlabel=category_features_day[6], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[6])

tgt = 2

fig, axes = plt.subplots(nrows=2,ncols=3)
fig.set_size_inches(25, 15)
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[0],orient="v",ax=axes[0][0])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[1],orient="v",ax=axes[0][1])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[2],orient="v",ax=axes[0][2])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[4],orient="v",ax=axes[1][0])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[5],orient="v",ax=axes[1][1])
sns.boxplot(data=day_df,y=target_features_day[tgt],x=category_features_day[6],orient="v",ax=axes[1][2])

txt = 'Box Plot on '

axes[0][0].set(xlabel=category_features_day[0], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[0])
axes[0][1].set(xlabel=category_features_day[1], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[1])
axes[0][2].set(xlabel=category_features_day[2], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[2])
axes[1][0].set(xlabel=category_features_day[4], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[4])
axes[1][1].set(xlabel=category_features_day[5], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[5])
axes[1][2].set(xlabel=category_features_day[6], ylabel=target_features_day[tgt],title= txt + target_features_day[tgt] + ' across ' + category_features_day[6])

nrows, ncols = 4, 3

fig, axes = plt.subplots(nrows=nrows,ncols=ncols)
fig.set_size_inches(25, 45)

year = 0

for i in range(nrows):
  for j in range(ncols):
    mnth = i*ncols + j + 1
    mnthplot = sns.barplot(data=day_df[np.logical_and(day_df['mnth'] == mnth ,day_df['yr'] == year)],
                y='cnt'      ,
                x='dteday'   ,
                orient="v"   ,
                ax=axes[i][j])
    mnthplot.set_xticklabels(mnthplot.get_xticklabels(),rotation=90)
    axes[i][j].set(ylim=(0, 7000))

"""# **Question 4**
From the plots, it can be observed that there are periodic dips across the months - this could be because during weekends the ridership falls. This makes sense as registered users are more than casual users.
"""

#OneHotEncoding
df_onehot = pd.get_dummies(day_df_drop[features], columns=['weathersit'],
                            drop_first=False)
df_onehot.info()

"""# **Question 6**
* Scalar encoding - Each unique category value is assigned an integer value. The integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.
For example,variables like “season”, "year", "month" above would be a good examples where a scalar encoding would be sufficient.

* One hot encoding - For categorical variables where no ordinal relationship exists, the scalar encoding is not enough. In fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results. In this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value. In this dataset, "weathersit" can be represented using one hot encoding.
"""

df_onehot.astype('category').describe()

df_onehot.info()

day_df_drop1=pd.concat([df_onehot,day_df[target_features_day]], axis=1)
day_df_drop1.info()

"""# **Question 7**

Standardizing feature columns using MinMaxScaler().
"""

mm_scaler = preprocessing.MinMaxScaler()
df_mm = mm_scaler.fit_transform(day_df_drop)

df_mm = pd.DataFrame(df_mm, columns=day_df_drop.columns)
#df_mm1=pd.concat([df_mm,day_df[target_features_day],day_df[category_features_day]], axis=1)

cols = [col for col in df_mm.columns if col not in ['casual','registered', 'cnt']]
cols_onehot = [col for col in day_df_drop1.columns if col not in ['casual','registered', 'cnt']]

data = df_mm[cols]
data_unscaled = day_df_drop1[cols_onehot]

colstar = [col for col in df_mm.columns if col in ['casual','registered', 'cnt']]
colstar_onehot = [col for col in day_df_drop1.columns if col in ['casual','registered', 'cnt']]

target = day_df_drop[colstar]
target_unscaled = day_df_drop1[colstar_onehot]
data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 10)
data_train_us, data_test_us, target_train_us, target_test_us = train_test_split(data_unscaled,target_unscaled, test_size = 0.20, random_state = 10)

"""# **Question 8**

By selecting the most important features, the test RMSE will be low. This is because, if the important features are not removed, the model will fit the unimportant features as well including any outlier cases - causing the test RMSE to be high. Thus feature selection improves model performance.
"""

from sklearn.feature_selection import f_regression, mutual_info_regression
#print(target)
col1 = [col for col in target_unscaled.columns if col in ['cnt']]
target1 = target_unscaled[col1]
#print(target1)
f_scores, p_vals = f_regression(data_unscaled, target1)
f_scores /= np.max(f_scores)
print(f_scores)
mi = mutual_info_regression(data_unscaled, target1)
mi /= np.max(mi)
print(mi)

daydf_std_col_list = day_df_drop1.columns.to_list()
print('Feature with max f-score :', daydf_std_col_list[f_scores.argmax()])
print('Feature with min f-score :', daydf_std_col_list[f_scores.argmin()])
print('Feature with max p-value :', daydf_std_col_list[p_vals.argmax()])
print('Feature with min p-value :', daydf_std_col_list[p_vals.argmin()])
print('Feature with max mi :', daydf_std_col_list[mi.argmax()])
print('Feature with min mi :', daydf_std_col_list[mi.argmin()])

"""# **Question 9**
The linear regression model is the simplest and the most commonly used prediction model. When the number of independent variables is larger than the number of observations, then this models are not valid mainly because there are infinite solutions to our estimators. 
Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.
In Ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients. The penalty term (lambda) regularizes the coefficients such that if the coefficients take large values the optimization function is penalized. So, ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity.
In Lasso regression, the only difference is instead of taking the square of the coefficients, magnitudes are taken into account. This type of regularization (L1) can lead to zero coefficients i.e. some of the features are completely neglected for the evaluation of output. So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection.

"""

from prettytable import PrettyTable
from sklearn.linear_model import Lasso, ElasticNet, Ridge, SGDRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error
from sklearn.linear_model import LinearRegression


table1 = PrettyTable()
table1.field_names = ["Unscaled Model", "Alpha", "Root Mean Squared Error"]

alphas = [0.001, 0.01, 0.1, 1, 10]
for alpha in alphas:
    models = [
        #SGDRegressor(max_iter=1000, tol=1e-3),
        Lasso(alpha=alpha),
        #ElasticNet(random_state=0),
        Ridge(alpha=alpha),
        LinearRegression()

        
    ]

    for model in models:
              
        cv_results = cross_validate(model, data_train, target_train, cv=10, scoring = 'neg_root_mean_squared_error', return_train_score=True)
        
        table1.add_row([type(model).__name__, format(alpha),format(-np.mean(cv_results['test_score']),'.4f')])

print(table1)

"""# **Question 10**

The best regularization scheme is Ridge regression as it has the lowest RMSE and the optimal penalty parameter here is 0.1. The parameter can be found by doing a grid search over the range of parameter values.

"""

table2 = PrettyTable()
table2.field_names = ["Scaled Model", "Alpha", "RMSE"]

alphas = [0.001, 0.01, 0.1, 1, 10]
for alpha in alphas:
   models1 = [        
        Lasso(alpha=alpha),        
        Ridge(alpha=alpha),
        LinearRegression()
        ]
 
   for i in models1:
      cv_results_1 = cross_validate(i, data_train_us, target_train_us, cv=10, scoring = 'neg_root_mean_squared_error', return_train_score=True)
        
      table2.add_row([type(i).__name__, format(alpha),format(-np.mean(cv_results_1['test_score']),'.4f')])

  

print(table2)

"""# **Question 11**
Scaling has a positive effect on learning as it reduces the RMSE value.

# **Question 12**

In statistical hypothesis testing, the p-value or probability value is the probability of obtaining test results at least as extreme as the results actually observed during the test, assuming that the null hypothesis is correct. \\
In this case, p-value is inversely proportional to the importance of features. The most significant features can be inferred from the table - they have the minimum p-values.
"""

import statsmodels.api as sm
from scipy import stats

est = sm.OLS(target1, data)
est2 = est.fit()
print(est2.summary())

from sklearn.preprocessing import PolynomialFeatures

table3 = PrettyTable()
table3.field_names = ["Model", "Degree", "Root Mean Squared Error"]

degrees = [1,2,3,4,5]

poly_feature_dict = {}

for degree in degrees:
    polynomial_features= PolynomialFeatures(degree=degree, interaction_only=True)
    data_poly = polynomial_features.fit_transform(data_train)
    poly_feature_list = polynomial_features.get_feature_names(list(data_train.columns))
    poly_feature_dict[str(degree)] = poly_feature_list
    data_poly_test = polynomial_features.fit_transform(data_test)
    model = LinearRegression()
    model.fit(data_poly, target_train)
    y_predicted_p = model.predict(data_poly_test)
    rmse = np.sqrt(mean_squared_error(target_test, y_predicted_p))
    #score = model.score(data_poly_test, target_test)  

    table3.add_row([type(model).__name__, format(degree, '.2f'), format(rmse, '.4f')])

print(table3)

"""# **Question 13**
The most salient features are given below. We observe more counts when holiday and weekday is combined this makes sense as when a holiday is observed on weekday may be many people opt for bikes. Also we got highest correlation for year and temperature in the heatmap so it makes intuitive sense to get it as salient feature.
"""

poly = PolynomialFeatures(3)
data_poly_np = poly.fit_transform(data_train)

poly_feature_list = poly.get_feature_names(list(data_train.columns))

data_poly = pd.DataFrame(data_poly_np, columns=poly_feature_list)
data_poly = data_poly.drop('1', axis=1)

col1 = [col for col in target_train.columns if col in ['cnt']]
target1 = target_train[col1]
f_scores, p_vals = f_regression(data_poly, target1)
#f_scores /= np.max(f_scores)
print(poly_feature_list[f_scores.argmax()])
print(poly_feature_list[p_vals.argmin()])
mi = mutual_info_regression(data_poly, target1)
#mi /= np.max(mi)
print(poly_feature_list[mi.argmax()])

"""# **Question 14**

Polynomial order of degree 3 is the best as it has the least RMSE. As the degree increases, the model will become too complex and it will start overfitting to the outliers. So it will not be generalized and test RMSE will be high.

# **Question 16**
Neural networks usually outperform linear regression as they deal with non linearities automatically, whereas in linear regression you need to mention explicitly. Because of this, neural networks are more generalized.
"""

from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import cross_validate

reg = MLPRegressor(hidden_layer_sizes=(400, 400), alpha=0.1, max_iter=1000)
cv_results_n = cross_validate(reg, data_train, target_train, cv=10, scoring = 'neg_root_mean_squared_error', return_train_score=True)
print(-np.mean(cv_results_n['train_score']))
print(-np.mean(cv_results_n['test_score']))

"""# **Question 17**

The neural network is optimized for the following parameters (obtained by trial and error):
* hidden layer sizes = 400x400
* alpha = 0.1
* max_iter = 1000

# **Question 18**
ReLU activation function can be used for the output as we want to map real values of the output to positive real values only because count is always postive.

# **Question 19**
Increasing depth can lead to following problems-
* be very difficult to train
* eventual overfit the training data completely
* vanishing and exploding gradients
* the greater the level of regularisation that is likely required to obtain a reasonable validation metric on unseen data.

# **Question 20**

* 'The number of trees' parameter is similar to ensembling in neural networks - This has a reqularization effect because it generalizes the model.

* The 'depth of trees': The deeper the tree, the more splits it has and it captures more information about the data. If the depth is too high, it can overfit the data.

* 'number of features': This is number of splits at each node. This relates to the complexity of the model.
"""

from sklearn import tree
from sklearn.ensemble import RandomForestRegressor
import graphviz 
# best tree: 3 features, 20 estimators,
RANDOM_STATE = 42
forest1 =  RandomForestRegressor(oob_score=True,
                               max_features=3,max_depth=4,
                               random_state=RANDOM_STATE)

forest1.set_params(n_estimators=20)
forest1.fit(data_train, target_train)
tree1 = forest1.estimators_[1]
dot_data = tree.export_graphviz(tree1, out_file=None, feature_names=features,
                                class_names="charges",   filled=True, rounded=True,  special_characters=True)
graph = graphviz.Source(dot_data)
graph.render('random_forest.png', view=False)
print("OOB score = " + str(forest1.oob_score_))

"""# **Question 21**
Random forests are bagged decision tree models that split on a subset of features on each split. It can handle binary features, categorical features, and numerical features. Random forests is great with high dimensional data since we are working with subsets of data. It is faster to train than decision trees because we are working only on a subset of features in this model, so we can easily work with hundreds of features. Prediction speed is significantly faster than training speed because we can save generated forests for future uses.
"""

from sklearn.model_selection import GridSearchCV

# Number of trees in random forest
n_estimators = np.arange(10, 50, 10, dtype=int)

# Number of features to consider at every split
max_features = np.arange(2, 10, 2, dtype=int)

# Maximum number of levels in tree
max_depth = np.arange(10, 51, 10, dtype=int)
#max_depth.append(None)

param_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth}

grid_cv = GridSearchCV(forest1, param_grid, cv=10)
grid_cv.fit(data_train,target_train)

print("Best Hyperparameters:\n{}".format(grid_cv.best_params_))

forest1 =  RandomForestRegressor(oob_score=True,
                               max_features=4,max_depth=10, n_estimators =40,
                               random_state=RANDOM_STATE)
cv_results_n = cross_validate(forest1, data_train, target_train, cv=10, scoring = 'neg_root_mean_squared_error', return_train_score=True)
print(-np.mean(cv_results_n['train_score']))
print(-np.mean(cv_results_n['test_score']))

"""# **Question 22**

The feature at the root node is a_temp. This is because it is the most important feature, as we got in section 3.2.1. The important features are a_temp and working day which matches our earlier results.

# **Question 23**
We performed 10 fold cross validation above for linear regression model, neural network and random forest. We obtained avg RMSE training and validation scores. We observe that training score is less than validation score this is because of over-fitting.

# **Question 24**
The R2 score tells you how successfully your model accounts for the intrinsic variation in the data. R2 score comes from running the trained model on the Validation data. The Out of bag (OOB) score is technically also an R2 score, because it uses the same mathematical formula; the Random Forest calculates it internally using only the Training data. Both scores predict the generalizability of your model – i.e. its expected performance on new, unseen data.
"""